



import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import (mean_absolute_error,
                             mean_squared_error,
                             r2_score)

from random import randint




#dataset after cleaning
db = pd.read_csv('../../datasets/processed/dataset-ECFPDescriptors.csv')




excluded_columns_names = ['molindx', 'logS']
target_vars = 'logS'

data_new = db.sample(frac=1)
number_of_training = 20




rnd_state = randint(1, 1000)
train_set, test_set = train_test_split(data_new,
                                       shuffle=True,
                                       random_state=rnd_state,
                                       test_size=0.15)

X_train = train_set.drop(columns=excluded_columns_names).to_numpy()
y_train = train_set[target_vars]

X_test = test_set.drop(columns=excluded_columns_names).to_numpy()
y_test = test_set[target_vars]

#RF hyperparameters optimization
model = RandomForestRegressor(oob_score=False, random_state = rnd_state)

n_estimators = range(100, 1001, 25)
max_features = np.arange(0.2, 0.99, 0.23)
min_samples_leaf = [2] #[2,3,4]
min_samples_split = [2] #[2,3,4]

parameters = {'n_estimators' : n_estimators,
              'max_features' : max_features,
              'min_samples_leaf' : min_samples_leaf,
              'min_samples_split' : min_samples_split,
             }

#gridsearch cross-validation for model's parameter optimization
rfr = GridSearchCV(model,
                   parameters,
                   cv=10,
                   verbose=1,
                   n_jobs=4,
                   scoring='neg_mean_squared_error')

rfr.fit(X_train, y_train.values.ravel())

model = rfr.best_estimator_
print('Best Model:', model)

y_pred = model.predict(X_test)

# Calculate the MAE
mae = mean_absolute_error(y_test, y_pred)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

r2 = r2_score(y_test, y_pred)


print(f'Mean Absolute Error: {mae:.2f} (mg/L)')
print(f'Mean Squared Error: {mse:.2f} (mg/L)$^{2}$')
print(f'Root Mean Squared Error: {rmse:.2f} (mg/L)')
print(f'R² Score: {r2:.2f}')




n_estimators = model.n_estimators
max_features = model.max_features




r2_list = []
mae_list = []
rmse_list = []

for nof_training in range(number_of_training):
    rnd_state = randint(1, 1000)
    train_set, test_set = train_test_split(data_new,
                                           shuffle=True,
                                           random_state=rnd_state,
                                           test_size=0.15)

    X_train = train_set.drop(columns=excluded_columns_names).to_numpy()
    y_train = train_set[target_vars]

    X_test = test_set.drop(columns=excluded_columns_names).to_numpy()
    y_test = test_set[target_vars]
    
    #RF hyperparameters optimization
    rfr_model = RandomForestRegressor(oob_score=False,
                                      n_estimators=n_estimators,
                                      max_features=max_features,
                                      random_state = rnd_state)

    rfr_model.fit(X_train, y_train.values.ravel())

    y_pred = rfr_model.predict(X_test)

    # Calculate the MAE
    mae = mean_absolute_error(y_test, y_pred)
    mae_list.append(mae)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    rmse_list.append(rmse)

    r2 = r2_score(y_test, y_pred)
    r2_list.append(r2)

mean_mae = np.mean(mae_list)
std_mae = np.std(mae_list)

mean_rmse = np.mean(rmse_list)
std_rmse = np.std(rmse_list)

mean_r2 = np.mean(r2_list)
std_r2 = np.std(r2_list)

print(f'Mean Absolute Error: {mean_mae:.2f} (mg/L) and std of MAE: {std_mae:.2f} (mg/L)')
print(f'Root Mean Squared Error: {mean_rmse:.2f} (mg/L) and std of RMSE: {std_rmse:.2f} (mg/L)')
print(f'R² Score: {mean_r2:.2f} and std of r2: {std_r2:.2f} (mg/L)')




#sctter plot
fig, ax = plt.subplots(figsize=(4, 4))

ax.scatter(y_pred, np.array(y_test), s=10, c='blue', facecolor='none', alpha=0.4)

ax.set_xlabel('Predicted values (mg/L)', fontsize=8)
ax.set_ylabel('True values (mg/L)', fontsize=8)

ymin = min(y_pred) - 10
ymax = max(y_pred) + 10
ngrids = 100
y_x = np.linspace(ymin, ymax, ngrids) 

ax.xaxis.set_major_locator(MultipleLocator(5))  
ax.yaxis.set_major_locator(MultipleLocator(5)) 

ax.xaxis.set_minor_locator(MultipleLocator(1))  
ax.yaxis.set_minor_locator(MultipleLocator(1))

stdVal = np.ones(ngrids) * np.std(y_test - y_pred)

plt.plot(y_x, y_x, 'k-')
plt.plot(y_x, y_x + stdVal, 'r--')
plt.plot(y_x, y_x - stdVal, 'r--')

plt.xlim(ymin, ymax)
plt.ylim(ymin, ymax)

plt.show()
plt.close()




